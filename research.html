---
layout: layout
title: "Research Overview"
---

<h2 id='publications' class="page-heading">Publications</h1>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/contract.png">
      <table>
        <tr>
          <td><a href="https://openreview.net/forum?id=mWN6cpA6Wr">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Hourglass Persistence for Graphs, Simplices, and Cells </b>
      <p> Mattie Ji*, <b>Indradyumna Roy*</b>, Vikas K Garg <br /> ICLR 2026  </p>
      <p>Unlike persistent homology, which observes topology only through growing subgraphs, we study how topology evolves under graph contractions. We develop a systematic expressivity theory, certified by minimal witness graphs and constructive separation proofs, together with practical algorithms. </p>
    </div>
  </div>


  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/ghash.png">
      <table>
        <tr>
          <td><a href="https://openreview.net/forum?id=HQcCd0laFq">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Exchangeability of GNN Representations with Applications to Graph Retrieval </b>
      <p> Kartik Nair, <b>Indradyumna Roy</b>, Soumen Chakrabarti, Anirban Dasgupta, Abir De <br /> ICLR 2026  </p>
      <p> We identify exchangeability as a fundamental property of neural representations under common design choices. Node embeddings in standard GNNs inherit this symmetry, enabling order-statistic approximations of general (possibly asymmetric) transportation-based graph similarities and a generalized LSH framework for subgraph matching and graph edit distance. </p>
    </div>
  </div>


  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/DISCo.png">
      <table>
        <tr>
          <td><a href="https://openreview.net/forum?id=cUdODCFjUM">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> A Dense Subset Index for Collective Query Coverage </b>
      <p> Kartik Nair, Pritish Chakraborty, Atharva Abhijit Tambat, <b>Indradyumna Roy</b>, Soumen Chakrabarti, Anirban Dasgupta, Abir De <br /> ICLR 2026  </p>
      <p> Modern retrieval setups often require multiple documents to collaborate for query coverage. We formulate this as a submodular optimization objective admitting a greedy algorithm, and reinterpret its marginal-gain maximization as a retrieval task. This leads to random-projectionâ€“based approximate data structures that enable scalable, sublinear-time collective retrieval with strong theoretical guarantees. </p>
    </div>
  </div>


  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/corgii_img.png">
      <table>
        <tr>          
          <td><a href="https://pritishc.com/posts/2025/10/corgii-blog/">Blog</a></td>
          <td><a href="https://arxiv.org/abs/2510.22479">Arxiv</a></td>
          <td><a href="https://openreview.net/forum?id=BGwZsFLJFU">Paper</a></td>
          <td><a href="https://github.com/structlearning/corgii">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Contextual Tokenization for Graph Inverted Indices</b> 
      <p> Pritish Chakraborty, <b>Indradyumna Roy</b>, Soumen Chakrabarti, Abir De <br /> NeurIPS 2025  </p>
      <p> CoRGII bridges the gap between neural and symbolic retrieval by combining the semantic fidelity of graph embeddings with the efficiency of inverted indices. It converts continuous graph features into discrete, indexable tokens and uses learned impact weighting and multi-probing to balance recall and speed. </p>
    </div>
  </div>



  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/leakage_img.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=GYZLed4d3M">Paper</a></td>
          <td><a href="https://github.com/Indradyumna/better-graph-matching">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Position: Graph Matching Systems Deserve Better Benchmarks </b> 
      <p> <b>Indradyumna Roy</b>, Saswat Meher, Eeshaan  Jain, Soumen Chakrabarti, Abir De <br /> ICML 2025  </p>
    <p> We uncover pervasive train-test leakage in graph similarity benchmarks; leveraging edit path invariances in  flexible cost Graph Edit Distance framework, we propose techniques for  scalable augmentation and adversarial testing to guide future models. </p>
    </div>
  </div>


  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/clique-demo.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=DFSb67ksVr">Paper</a></td>
          <td><a href="https://github.com/structlearning/mxnet">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations </b> 
      <p> <b>Indradyumna Roy*</b>, Eeshaan  Jain*, Soumen Chakrabarti, Abir De <br /> ICLR 2025  </p>
    <p> MxNet is a fully differentiable clique number estimator that learns from distant supervision without explicit clique demonstrations. We reformulate MCP as detecting dense submatrices via learned permutations within a nested subgraph matching task. </p>
    </div>
  </div>
 
  <div class="divider"></div>

  <div class="row"> 
    <div class="six columns">
      <img style="margin-top:0em" src="/images/Data-Paper.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=5pd78GmXC6">Paper</a></td>
          <td><a href="https://github.com/structlearning/neural-subm-design-space">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Charting the Design Space of Neural Graph Representations for Subgraph Matching </b> 
      <p>  Vaibhav Raj*, <b>Indradyumna Roy*</b>, Ashwin Ramachandran, Soumen Chakrabarti, Abir De <br /> ICLR 2025  </p>
    <p> We present a unified design space for neural subgraph matching, uncovering hiterto unexplored design combinations that significantly boost performance. Our study provides key insights and general principles for neural graph representation and interaction.</p>
    </div>
  </div>

  
  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/GRAPHEDX.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=u7JRmrGutT">Paper</a></td>
	        <td><a href="https://github.com/structlearning/GraphEdX">Code</a></td>
          <td><a href="https://arxiv.org/abs/2409.17687">Arxiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Graph Edit Distance  with General Costs  Using Neural Set Divergence </b> 
      <p> Eeshaan  Jain*, <b>Indradyumna Roy*</b>, Saswat Meher, Soumen Chakrabarti, Abir De <br /> NeurIPS 2024  </p>
    <p> GraphEdx is the first-of-its-kind neural GED framework that incorporates variable edit costs, capable of modeling both symmetric and asymmetric graph (dis)similarities, allowing for more flexible and accurate GED estimation compared to earlier methods. </p>
    </div>
  </div>
 
  <div class="divider"></div>

  <div class="row"> 
    <div class="six columns">
      <img style="margin-top:0em" src="/images/EINSMATCH.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=udTwwF7tks">Paper</a></td>
	        <td><a href="https://github.com/structlearning/isonetpp">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval </b> 
      <p> Ashwin Ramachandran*, Vaibhav Raj*, <b>Indradyumna Roy</b>, Soumen Chakrabarti, Abir De <br /> NeurIPS 2024  </p>
    <p> IsoNet++ is an early interaction graph neural network,  where the approximate injective alignments between any given graph pair gets progressively refined with successive rounds, resulting in significantly better retrieval performance than existing methods. </p>
    </div>
  </div>


  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/FOURIERHASHNET.png">
      <table>
        <tr>          
	<!--<td><a href="https://arxiv.org/abs/2012.08974">Arxiv</a></td>-->
          <td><a href="/pdfs/2023_FourierHashNet.pdf">Paper</a></td>
	  <!--<td><a href="/pdfs/IsoNet_supp.pdf">Supplementary</a></td>-->
	  <td><a href="https://github.com/Indradyumna/FourierHashNet">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search </b> 
      <p> <b>Indradyumna Roy</b>, Rishi Agarwal, Soumen Chakrabarti, Anirban Dasgupta, Abir De <br /> NeurIPS 2023  (<span style='color:red'>Spotlight</span>)  </p>
    <p> FourierHashNet is an asymmetric LSH for hinge distance, which first transforms the hinge distance into a bounded dominance similarity measure, which is then Fourier-transformed into an expectation of inner products of functions in the frequency domain. Finally, the expectations are approximated with an importance-sampled estimate, which allows for the use of traditional Random-Hyperplanes LSH. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/MCSNET.png">
      <table>
        <tr>          
	<!--<td><a href="https://arxiv.org/abs/2012.08974">Arxiv</a></td>-->
          <td><a href="/pdfs/2022_NeuralMcs.pdf">Paper</a></td>
	  <!--<td><a href="/pdfs/IsoNet_supp.pdf">Supplementary</a></td>-->
          <td><a href="https://github.com/Indradyumna/MCSNET">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Maximum Common Subgraph Guided Graph Retrieval: Late and Early Interaction Networks </b> 
      <p> <b>Indradyumna Roy</b>, Soumen Chakrabarti, Abir De <br /> NeurIPS 2022 </p>
    <p> We propose neural architectures for two distinct variants of the MCS metric. The customized late interaction models for each variant, outperform SOTA in terms of retrieval accuracy and speed. Furthermore, an unified early interaction network is proposed, which works well for both variants and affords an additional boost in accuracy at the cost of some retrieval speed.  </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/ISONET_img.png">
      <table>
        <tr>          
	<!--<td><a href="https://arxiv.org/abs/2012.08974">Arxiv</a></td>-->
          <td><a href="/pdfs/IsoNet_main.pdf">Main</a></td>
          <td><a href="/pdfs/IsoNet_supp.pdf">Supplementary</a></td>
          <td><a href="https://github.com/Indradyumna/ISONET">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Interpretable Neural Subgraph Matching for Graph Retrieval </b>
      <p> <b>Indradyumna Roy</b>, Venkata Sai Velugoti, Soumen Chakrabarti, Abir De <br /> AAAI 2022 </p>
    <p> ISONET proposes a novel interpretable neural edge alignment formulation, which enables identification of the underlying subgraph in a corpus graph, which is relevant (isomorphic) to the given query graph. Training for ISONET is done using only binary relevance lavels on graph pairs, without any fine-grained ground truth information about node or edge alignments. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/PermGNN.png">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/abs/2012.08974">Arxiv</a></td>
          <td><a href="https://www.youtube.com/watch?v=i7WOrkwSL24&t=103s">Talk</a></td>
          <td><a href="https://github.com/Indradyumna/PermGNN">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Adversarial Permutation Guided Node Representations for Link Prediction </b>
      <p> <b>Indradyumna Roy</b>, Abir De, Soumen Chakrabarti <br /> AAAI 2021 </p>
    <p> PermGNN casts the link prediction objective as an adversarial game, which allows for usage of order-sensitive RNNs as neighborhood feature aggregators. We ensure permutation insensitivity by optimizing a min-max ranking loss function with respect to the smooth surrogates of adversarial permutations. </p>
    </div>
  </div>

  <div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/Plagiarism.png">
      <table>
        <tr>          
          <td><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2012/de12_interspeech.pdf">Paper</a></td>
          <td><a href="https://arxiv.org/abs/1503.00022">Arxiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Plagiarism Detection In Polyphonic Music Using Monaural Signal Separation </b>
      <p> Soham De, <b>Indradyumna Roy</b>, Tarunima Prabhakar, Kriti Suneja, Sourish Chaudhuri, Rita Singh, Bhiksha Raj<br /> INTERSPEECH 2012 </p>
    <p>  We present a novel feature space for audio derived from compositional modelling techniques, commonly used in signal separation, that provides a mechanism to account for polyphony without incurring an inordinate amount of computational overhead. </p>
    </div>
  </div>

  <div class="divider"></div>

